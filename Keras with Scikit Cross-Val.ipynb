{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with SciKit GridSearchCV\n",
    "\n",
    "This notebook will show how to perform hyperparamter tuning for deep learning models built using Keras. The GridSearchCV function from SciKit learn can be used to optimize all tunable parameters. The following list are some tunable parameters for an MLP:\n",
    "\n",
    "* Number Hidden Neurons per layer\n",
    "* Activation functions\n",
    "* Initialisations\n",
    "* Optimizers\n",
    "* Learning rate\n",
    "* Decay\n",
    "* Momentum\n",
    "* Dropout\n",
    "\n",
    "Keras is a library that enables deep learning practisioners to build multi layered deep learning models with minimal code. See the Keras Intro notebook for an introduction to the Keras library.\n",
    "\n",
    "SciKit-Learn is the go-to machine learning library in Python, this notebook shows how to combine the functionality of SciKit-Learn with models built in Keras. This is especially useful for performing k-fold cross validation, building pipelines and optimising hyper parameters.\n",
    "\n",
    "I will use the wines dataset from 1994 and predict the class of wine from its set of attributes. This dataset can be obtained from the UCI machine learning archive. This is a very small dataset which will allow us time to perform extensive grid search optimisation.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>14.23</th>\n",
       "      <th>1.71</th>\n",
       "      <th>2.43</th>\n",
       "      <th>15.6</th>\n",
       "      <th>127</th>\n",
       "      <th>2.8</th>\n",
       "      <th>3.06</th>\n",
       "      <th>.28</th>\n",
       "      <th>2.29</th>\n",
       "      <th>5.64</th>\n",
       "      <th>1.04</th>\n",
       "      <th>3.92</th>\n",
       "      <th>1065</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  14.23  1.71  2.43  15.6  127   2.8  3.06   .28  2.29  5.64  1.04  3.92  \\\n",
       "0  1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "1  1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "2  1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "3  1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "4  1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97  6.75  1.05  2.85   \n",
       "\n",
       "   1065  \n",
       "0  1050  \n",
       "1  1185  \n",
       "2  1480  \n",
       "3   735  \n",
       "4  1450  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    71\n",
       "0    58\n",
       "2    48\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Class','Alcohol', 'Malic acid', 'Ash','Alcalinity of ash', 'Magnesium', 'Total phenols',\\\n",
    "        'Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines',\\\n",
    "        'Proline']\n",
    "\n",
    "df.columns = cols\n",
    "\n",
    "df.head()\n",
    "\n",
    "class_ind = np.sort(df.Class.value_counts().index)\n",
    "\n",
    "if class_ind[0] == 1:\n",
    "    class_convert = {1:0, 2:1, 3:2}\n",
    "    df.Class = df.Class.map(lambda x: class_convert[x])\n",
    "\n",
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Feature data (X) and labels (y)\n",
    "\n",
    "Split data into training and test sets using sklearns train_test_split function\n",
    "\n",
    "Convert dataframes to numpy arrays and one hot encode class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 1 --- One hot encoded: [ 0.  1.  0.]\n",
      "Original label: 1 --- One hot encoded: [ 0.  1.  0.]\n",
      "Original label: 2 --- One hot encoded: [ 0.  0.  1.]\n",
      "Original label: 1 --- One hot encoded: [ 0.  1.  0.]\n",
      "Original label: 1 --- One hot encoded: [ 0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "X = df.drop(['Class'], axis=1)\n",
    "y = df.Class\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=45)\n",
    "\n",
    "X_train, X_test, y_train = np.asarray(X_train), np.asarray(X_test), np.asarray(y_train)\n",
    "\n",
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Original label: {0} --- One hot encoded: {1}\".format(y_train[i], y_train_ohe[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code for understanding how Keras uses scikit-learn functionality can be found here:\n",
    "https://github.com/fchollet/keras/blob/master/keras/wrappers/scikit_learn.py\n",
    "\n",
    "The BaseWrapper class serves as a parent class, it is recommended to call a descendent class that will in turn inherit the parent class properties. This allows separation of KerasRegressor and KerasClassifier.\n",
    "\n",
    "The KerasCLassifier which we will use here takes the argument 'build_fn'. Using this, we can define our own function, build a MLP within and pass the model to KerasClassifier.\n",
    "\n",
    "Param grid takes a dictionary of parameters to pass to gridsearch which will then form all possible HP combinations before performing 3-fold CV on them. For additional GridSearchCV functionality see the documentation:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Set a random seed so the model is replicable \n",
    "np.random.seed(0) \n",
    "\n",
    "def call_model(optimizer = 'sgd', init='uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=X_train.shape[1], \n",
    "                output_dim=50, \n",
    "                init=init, \n",
    "                activation='tanh'))\n",
    "    model.add(Dense(input_dim=50, \n",
    "                output_dim=50, \n",
    "                init=init, \n",
    "                activation='tanh'))\n",
    "    model.add(Dense(input_dim=50, \n",
    "                output_dim=y_train_ohe.shape[1], \n",
    "                init=init, \n",
    "                activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Call instance of the model\n",
    "model = KerasClassifier(build_fn=call_model)\n",
    "\n",
    "# grid search epochs, batch size and initialisations. ALL HP below can be extended. Keras documentation provides list of options\n",
    "optimizers = ['rmsprop']\n",
    "init = ['normal']\n",
    "epochs = [25,50]\n",
    "batches = [10, 20]\n",
    "\n",
    "# For this case I have chosen only 4 HP combinations to attempt\n",
    "param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init, verbose=[1])\n",
    "\n",
    "gscv = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "model_fit = gscv.fit(X_train, y_train_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can evaluate the grid search in much the same way that we would choose to evaluate a grid search using models built from sklearns own library.\n",
    "\n",
    "Best result can be printed using best\\_score\\_ and best params can be found from best\\_params\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.9119 using {'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10, 'init': 'normal', 'verbose': 1}\n",
      "\n",
      "Mean Score: 0.7987  StDev: 0.0695 with: \n",
      " {'optimizer': 'rmsprop', 'nb_epoch': 25, 'batch_size': 10, 'init': 'normal', 'verbose': 1}\n",
      "Mean Score: 0.9119  StDev: 0.0471 with: \n",
      " {'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10, 'init': 'normal', 'verbose': 1}\n",
      "Mean Score: 0.7358  StDev: 0.0308 with: \n",
      " {'optimizer': 'rmsprop', 'nb_epoch': 25, 'batch_size': 20, 'init': 'normal', 'verbose': 1}\n",
      "Mean Score: 0.8553  StDev: 0.0728 with: \n",
      " {'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 20, 'init': 'normal', 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: {:.4f} using {}\\n\".format(model_fit.best_score_, model_fit.best_params_))\n",
    "for params, mean_score, scores in model_fit.grid_scores_:\n",
    "    print(\"Mean Score: {:.4f}  StDev: {:.4f} with: \\n {}\".format(scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score achieved here is 0.9119, this can definitely be improved by increasing the number of epochs.\n",
    "\n",
    "Testing on the test set yields the score shown below. This is quite a poor score for the dataset, simple regression and clustering models score in excess of 95% but again the purpose of this notebook was to demonstrate the ability to use sklearn functionality with Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/18 [===============>..............] - ETA: 0s\n",
      "\n",
      "Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "pred = model_fit.predict(X_test)\n",
    "accuracy = sum(pred == y_test) / len(y_test)\n",
    "print(\"\\n\\nAccuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HP Optimisation: Layered tuning\n",
    "\n",
    "The following code repeats the procedure outlined above but this time fine tunes parameters within the models itself such as number of hidden neurons and activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set a random seed so the model is replicable \n",
    "np.random.seed(0) \n",
    "\n",
    "def call_model(optimizer = 'rmsprop', init='normal', activation='tanh', output_dim=50):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=X_train.shape[1], \n",
    "                output_dim = output_dim[0], \n",
    "                init=init, \n",
    "                activation = activation[0]))\n",
    "    model.add(Dense(input_dim= output_dim[0], \n",
    "                output_dim = output_dim[1], \n",
    "                init=init, \n",
    "                activation= activation[1]))\n",
    "    model.add(Dense(input_dim = output_dim[1], \n",
    "                output_dim = output_dim[2], \n",
    "                init=init, \n",
    "                activation=activation[2]))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Call instance of the model\n",
    "model = KerasClassifier(build_fn=call_model)\n",
    "\n",
    "# This time we are casting a grid search over activation functions and number of neurons per layer\n",
    "epochs = [50]\n",
    "batches = [15]\n",
    "activations = [['tanh','tanh','softmax'],['sigmoid','sigmoid','softmax']]\n",
    "outputs = [[50,50,3],[75,75,3]]\n",
    "\n",
    "param_grid = dict(nb_epoch=epochs, batch_size=batches, activation=activations, output_dim = outputs, verbose=[1])\n",
    "\n",
    "gscv = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "model_fit = gscv.fit(X_train, y_train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8113 using {'output_dim': [50, 50, 3], 'activation': ['tanh', 'tanh', 'softmax'], 'batch_size': 15, 'nb_epoch': 50, 'verbose': 1}\n",
      "\n",
      "Mean Score: 0.8113  StDev: 0.0924 with: \n",
      " {'output_dim': [50, 50, 3], 'activation': ['tanh', 'tanh', 'softmax'], 'batch_size': 15, 'nb_epoch': 50, 'verbose': 1}\n",
      "Mean Score: 0.7673  StDev: 0.0583 with: \n",
      " {'output_dim': [75, 75, 3], 'activation': ['tanh', 'tanh', 'softmax'], 'batch_size': 15, 'nb_epoch': 50, 'verbose': 1}\n",
      "Mean Score: 0.6792  StDev: 0.0408 with: \n",
      " {'output_dim': [50, 50, 3], 'activation': ['sigmoid', 'sigmoid', 'softmax'], 'batch_size': 15, 'nb_epoch': 50, 'verbose': 1}\n",
      "Mean Score: 0.6792  StDev: 0.0308 with: \n",
      " {'output_dim': [75, 75, 3], 'activation': ['sigmoid', 'sigmoid', 'softmax'], 'batch_size': 15, 'nb_epoch': 50, 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: {:.4f} using {}\\n\".format(model_fit.best_score_, model_fit.best_params_))\n",
    "for params, mean_score, scores in model_fit.grid_scores_:\n",
    "    print(\"Mean Score: {:.4f}  StDev: {:.4f} with: \\n {}\".format(scores.mean(), scores.std(), params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

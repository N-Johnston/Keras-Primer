{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout with Keras\n",
    "\n",
    "This notebook shows how to add dropout layers to Keras models in order to include the dropout regularisation technique when training neural networks.\n",
    "\n",
    "Dropout is a regularisation technique introduced in 2014 by Srivastava et al. The original publication can be found here in pdf format: http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\n",
    "The authors of this paper found that when dropout was included in the training process, the generalisation of the models were improved. They prove this by besting some benchmark scores on well known training sets including MNIST and CIFAR10.\n",
    "\n",
    "The images below show the basic principle and have been taken directly from the published paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Pic](Images/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Dropout Works\n",
    "\n",
    "As the above image shows the term 'dropout' simply refers to the visible or hidden units within the neural network. A unit is temporarily removed from the network, including its connected weights. Units are chosen at random, with fixed P, independent of other units. The P of dropout can be chosen by the user for each layer. Srivastava et al state that visible units should have a P of retention close to 1 while 0.5 is optimal for hidden layers.\n",
    "\n",
    "Training with dropout is in theory the same as training 2^n different networks. Dropout is applied to the neural net at the beginning of each training pass. This means each neural network gets trained very rarely or not at all. At test time the outgoing weight of a unit is multiplied by its rentention probability. This ensures the model used for testing matches the expected output during training. This is analogous to combining all 2^n models into one final test model.\n",
    "\n",
    "An additional aspect noted by the authors, specific to the backprop algorithm was the combination of dropout with max-norm constraints. Max-norm constraints restrict wieght vectors from becoming too large by capping an upper bound on the magnitude of the incoming weight vector for each neuron. If a gradient descent step moves the weight vector so that its magnitude ||w||2 becomes greater than the constraint C, the weight vector is projected back onto a ball of radius C. This prevents the weight vectors growing out of control, as can happen when using a large learning rate.\n",
    "\n",
    "The code below uses the MNIST dataset. The first model is an original build of a neural network optimized with stochastic gradient descent. The second model shows the implementation of dropout and maxnorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score as cv\n",
    "\n",
    "def feature_label_split(path, data='train'):\n",
    "    df = pd.read_csv(os.path.join(path,'mnist_{}.csv'.format(data)), header=None)\n",
    "    y = df.iloc[:,0]\n",
    "    X = df.drop(df.columns[0], axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: Rows, Columns\n",
      "Feature set (60000, 784) and labels (60000,)\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "X_train, y_train = feature_label_split(cwd, data='train')\n",
    "print(\"Training Data: Rows, Columns\")\n",
    "print(\"Feature set {0} and labels {1}\".format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data: Rows, Columns\n",
      "Feature set (10000, 784) and labels (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = feature_label_split(cwd, data='test')\n",
    "print(\"Test Data: Rows, Columns\")\n",
    "print(\"Feature set {0} and labels {1}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test = np.asarray(X_train), np.asarray(X_test)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 5 --- One hot encoded: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "Original label: 0 --- One hot encoded: [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Original label: 4 --- One hot encoded: [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "Original label: 1 --- One hot encoded: [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Original label: 9 --- One hot encoded: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "y_train_ohe = np_utils.to_categorical(y_train)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Original label: {0} --- One hot encoded: {1}\".format(y_train[i], y_train_ohe[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP - BackProp\n",
    "\n",
    "Trained with 5-fold cross-validation. Recommended cv split is typically 10 fold, restricted here as the code is slow to run using the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set a random seed so the model is replicable \n",
    "np.random.seed(0) \n",
    "\n",
    "def mnist_NN_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=X_train.shape[1], \n",
    "                output_dim=50, \n",
    "                init='uniform', \n",
    "                activation='tanh'))\n",
    "    model.add(Dense(input_dim=50, \n",
    "                output_dim=50, \n",
    "                init='uniform', \n",
    "                activation='tanh'))\n",
    "    model.add(Dense(input_dim=50, \n",
    "                output_dim=y_train_ohe.shape[1], \n",
    "                init='uniform', \n",
    "                activation='softmax'))\n",
    "    sgd = SGD(lr=0.001, decay=1e-7, momentum=.9, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "kf = StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Call instance of the model\n",
    "model = KerasClassifier(build_fn=mnist_NN_model, nb_epoch = 50, batch_size=300, verbose=0)\n",
    "\n",
    "results = cv(model, X_train, y_train_ohe, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.16%, Std: 0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%, Std: {:.2f}\".format(results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - BackProp with Dropout and max-norm\n",
    "\n",
    "Dropout can be applied to any layer, simply add a new layer from .add()\n",
    "\n",
    "max-norm is applied when adding layers and is one of the kwargs of Dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set a random seed so the model is replicable \n",
    "np.random.seed(0) \n",
    "\n",
    "def mnist_NN_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.1, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(output_dim=50, \n",
    "                init='uniform', \n",
    "                activation='tanh',\n",
    "                W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(input_dim=50, \n",
    "                output_dim=50, \n",
    "                init='uniform', \n",
    "                activation='tanh',\n",
    "                W_constraint=maxnorm(3)))\n",
    "    model.add(Dense(input_dim=50, \n",
    "                output_dim=y_train_ohe.shape[1], \n",
    "                init='uniform', \n",
    "                activation='softmax'))\n",
    "    sgd = SGD(lr=0.001, decay=1e-7, momentum=.9, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "kf = StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Call instance of the model\n",
    "model = KerasClassifier(build_fn=mnist_NN_model, nb_epoch = 50, batch_size=300, verbose=0)\n",
    "\n",
    "results = cv(model, X_train, y_train_ohe, cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.00%, Std: 0.38\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.2f}%, Std: {:.2f}\".format(results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example I actually managed to decrease the mean accuracy. This is most likely due to a poor choice for the max-norm constant. With proper HP optimisation I believe the scores would improve as proven in the original paper. The Std is a multiple of 9 x bigger than the non regularised model. This could be an indication that the 'basic' model is slightly overfitting the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
